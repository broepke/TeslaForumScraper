---
title: "Tesla Forum Web Scraper"
author: "Brian Roepkee"
date: "Dec 8, 2020"
output:
  html_document:
    df_print: paged
---

# Background
Techniques in this notebook were built off of the learnings from the Data Camp course on Web Scraping with R.  This takes into recommendation from DataCamp the guidance to "Be Kind" to web servers, therefore it includes delays before each scraping call to pages using the `slowly` function from `purrr`.   

https://learn.datacamp.com/courses/web-scraping-in-r

```{r message=FALSE, warning=FALSE}
library(tidyverse) 
library(rvest)
library(stringi)
library(purrr)
library(robotstxt)
```

**Note:** Check to See if Scraping is Acceptable

```{r}
paths_allowed(
    path = "/categories/tesla-model-3/",
    domain = "https://forums.tesla.com/")
```

# Tesla Model 3

## Get URLs
Get a list of main topics per Page. This takes a text string URL and extracts all pages and URLS from it.

```{r}
get_topics_and_urls <- function(html){
  
  #Get a clean text version of all the topics
  topics <- html %>%
    html_nodes(xpath = '//html/body/div/main/ul/li/div/div[1]') %>%
    html_text(trim = TRUE)
  
  # Get the corresponding URLs
  topic_urls <- html %>%
    html_nodes(xpath = '//html/body/div/main/ul/li/div/div[1]/a') %>%
    html_attr("href")
  
  #Combine these into a single Tibble to return
  main_topics <- tibble(Topic = topics,
                        URLs = topic_urls)
}
```

## Get the Total Number of Pages

Generalized Utility function for returning the number of pages from the pager.  This works on the main Topics page as well as the discussion thread pages.

Note: URL Page Structure adds a `p#` after the end of the URL like this:

https://forums.tesla.com/categories/tesla-model-3/p2

```{r}
# Returns the total number of pages from the pager
get_last_page <- function(html){
    
    last_page <- html %>%
      html_node(".LastPage") %>%
      html_text()
    
    if(is.na(last_page)){
      last_page <-  1
    }
    
    last_page
}
```


# Discussion Pages
Each one of these should be identical. 

https://forums.tesla.com/discussion/59386/please-read-first-before-posting-on-the-forums/p2

## Get Discussion Text Per Page

```{r}
get_discussion_page_content <- function(html, title){
  
  discussion <- html %>%
    html_nodes(".userContent") %>%
    html_text(trim = TRUE)
  
  time <- html %>%
    html_nodes("time") %>%
    html_text(trim = TRUE)
  
  # Extract the user names from each post
  user <- html %>%
    html_nodes(".js-userCard") %>%
    html_text(trim = TRUE) %>%
    stri_remove_empty_na()
  
  # Combine all of this into a tibble.
  discussion <- tibble(Topic = title,
                       Discussion = discussion,
                       User = user,
                       Time = time)  
}
```

## Loop Through the Discussion and

```{r}
get_all_discussions <- function(url){
  
  html <- read_html(url)
  
  # Get the discussion title
  discussion_title <- html %>%
    html_nodes("h1:nth-child(2)") %>%
    html_text(trim = TRUE)
  
  # Get the total number of pages per topic
  num_pages <- get_last_page(html)
  
  
  # Construct the URLs to query
  page_urls <- c(paste0(url,"p", 1:num_pages))
  
  # Define a throttled read_html() function with a delay of N seconds.
  read_html_delayed <- slowly(read_html, rate = rate_delay(0.1))
  
  discussion_threads <- tibble()

  for (page_url in page_urls){
    html <- read_html_delayed(page_url)
    page_data <- get_discussion_page_content(html, discussion_title)
    discussion_threads <- rbind(discussion_threads, page_data)
  }
  
  discussion_threads
}
```


# Actual Scraping

## Start by getting a list of all the page URLs.  
**Note:** The depth is controlled by the number in the paste0 function. Currently set up to give 3 URLs / Pages.

```{r}
page_urls <- c(paste0("https://forums.tesla.com/categories/tesla-model-3/p", 1:1))


# Define a throttled read_html() function with a delay of N seconds.
read_html_delayed <- slowly(read_html, rate = rate_delay(0.1))

#initialize a blank tibble for the incoming data (for rbind)
model3_topics <- tibble()

for (page_url in page_urls){
  html <- read_html_delayed(page_url)
  page_data <- get_topics_and_urls(html)
  model3_topics <- rbind(model3_topics, page_data)
}
```

## Get All Topics Based on URLs from the Step Above

```{r}
topics_data <- tibble()

for (topic in model3_topics$URLs){
  topic_data <- get_all_discussions(topic)
  topics_data <- rbind(topics_data, topic_data)
  # write the file after each loop.  You can see the progress
  # and if something fails you don't have to start over necessarily.
  write_csv(topics_data, "tesla_discussions.csv")
}
```

